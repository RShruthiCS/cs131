{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f371cd-fc65-4fcb-8422-fbe3826ae1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 scripts/filter_tech.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2afeafe-36df-48d6-9014-ae3167b032de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/30 05:38:56 INFO SparkContext: Running Spark version 4.0.0\n",
      "25/07/30 05:38:56 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, aarch64\n",
      "25/07/30 05:38:56 INFO SparkContext: Java version 17.0.16\n",
      "25/07/30 05:38:56 INFO ResourceUtils: ==============================================================\n",
      "25/07/30 05:38:56 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/07/30 05:38:56 INFO ResourceUtils: ==============================================================\n",
      "25/07/30 05:38:56 INFO SparkContext: Submitted application: TechStockAnalysis\n",
      "25/07/30 05:38:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/07/30 05:38:57 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/07/30 05:38:57 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/07/30 05:38:57 INFO SecurityManager: Changing view acls to: spark\n",
      "25/07/30 05:38:57 INFO SecurityManager: Changing modify acls to: spark\n",
      "25/07/30 05:38:57 INFO SecurityManager: Changing view acls groups to: spark\n",
      "25/07/30 05:38:57 INFO SecurityManager: Changing modify acls groups to: spark\n",
      "25/07/30 05:38:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY; RPC SSL disabled\n",
      "25/07/30 05:38:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/30 05:38:57 INFO Utils: Successfully started service 'sparkDriver' on port 32831.\n",
      "25/07/30 05:38:57 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/07/30 05:38:57 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/07/30 05:38:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/07/30 05:38:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/07/30 05:38:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/07/30 05:38:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0e5c3a88-0c9e-46f9-bc13-8147e773e2bb\n",
      "25/07/30 05:38:57 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/07/30 05:38:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/07/30 05:38:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/30 05:38:57 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "25/07/30 05:38:57 INFO SecurityManager: Changing view acls to: spark\n",
      "25/07/30 05:38:57 INFO SecurityManager: Changing modify acls to: spark\n",
      "25/07/30 05:38:57 INFO SecurityManager: Changing view acls groups to: spark\n",
      "25/07/30 05:38:57 INFO SecurityManager: Changing modify acls groups to: spark\n",
      "25/07/30 05:38:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY; RPC SSL disabled\n",
      "25/07/30 05:38:58 INFO Executor: Starting executor ID driver on host b7942ec7cbdb\n",
      "25/07/30 05:38:58 INFO Executor: OS info Linux, 6.10.14-linuxkit, aarch64\n",
      "25/07/30 05:38:58 INFO Executor: Java version 17.0.16\n",
      "25/07/30 05:38:58 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/07/30 05:38:58 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4b950330 for default.\n",
      "25/07/30 05:38:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37199.\n",
      "25/07/30 05:38:58 INFO NettyBlockTransferService: Server created on b7942ec7cbdb:37199\n",
      "25/07/30 05:38:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/07/30 05:38:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b7942ec7cbdb, 37199, None)\n",
      "25/07/30 05:38:58 INFO BlockManagerMasterEndpoint: Registering block manager b7942ec7cbdb:37199 with 434.4 MiB RAM, BlockManagerId(driver, b7942ec7cbdb, 37199, None)\n",
      "25/07/30 05:38:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b7942ec7cbdb, 37199, None)\n",
      "25/07/30 05:38:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b7942ec7cbdb, 37199, None)\n",
      "25/07/30 05:38:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/07/30 05:38:59 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.\n",
      "25/07/30 05:39:00 INFO InMemoryFileIndex: It took 25 ms to list leaf files for 1 paths.\n",
      "25/07/30 05:39:00 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "25/07/30 05:39:01 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/07/30 05:39:01 INFO FileSourceStrategy: Post-Scan Filters: Set((length(trim(value#0, None)) > 0))\n",
      "25/07/30 05:39:01 INFO CodeGenerator: Code generated in 155.609125 ms\n",
      "25/07/30 05:39:01 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "25/07/30 05:39:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 214.3 KiB, free 434.2 MiB)\n",
      "25/07/30 05:39:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.7 KiB, free 434.2 MiB)\n",
      "25/07/30 05:39:01 INFO SparkContext: Created broadcast 0 from csv at <unknown>:0\n",
      "25/07/30 05:39:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/30 05:39:01 INFO SparkContext: Starting job: csv at <unknown>:0\n",
      "25/07/30 05:39:01 INFO DAGScheduler: Got job 0 (csv at <unknown>:0) with 1 output partitions\n",
      "25/07/30 05:39:01 INFO DAGScheduler: Final stage: ResultStage 0 (csv at <unknown>:0)\n",
      "25/07/30 05:39:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/30 05:39:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:01 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.3 KiB, free 434.1 MiB)\n",
      "25/07/30 05:39:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.1 MiB)\n",
      "25/07/30 05:39:01 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/30 05:39:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/07/30 05:39:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (b7942ec7cbdb,executor driver, partition 0, PROCESS_LOCAL, 10228 bytes) \n",
      "25/07/30 05:39:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/07/30 05:39:01 INFO CodeGenerator: Code generated in 7.107375 ms\n",
      "25/07/30 05:39:01 INFO FileScanRDD: Reading File path: file:///app/data/tech_filtered.csv, range: 0-148812, partition values: [empty row]\n",
      "25/07/30 05:39:01 INFO CodeGenerator: Code generated in 7.390542 ms\n",
      "25/07/30 05:39:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1881 bytes result sent to driver\n",
      "25/07/30 05:39:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 226 ms on b7942ec7cbdb (executor driver) (1/1)\n",
      "25/07/30 05:39:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:02 INFO DAGScheduler: ResultStage 0 (csv at <unknown>:0) finished in 326 ms\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/30 05:39:02 INFO TaskSchedulerImpl: Canceling stage 0\n",
      "25/07/30 05:39:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Job 0 finished: csv at <unknown>:0, took 354.516834 ms\n",
      "25/07/30 05:39:02 INFO CodeGenerator: Code generated in 3.787167 ms\n",
      "25/07/30 05:39:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/07/30 05:39:02 INFO FileSourceStrategy: Post-Scan Filters: Set()\n",
      "25/07/30 05:39:02 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 214.3 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:02 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 38.7 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:02 INFO SparkContext: Created broadcast 2 from csv at <unknown>:0\n",
      "25/07/30 05:39:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/30 05:39:02 INFO SparkContext: Starting job: csv at <unknown>:0\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Got job 1 (csv at <unknown>:0) with 1 output partitions\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Final stage: ResultStage 1 (csv at <unknown>:0)\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 22.8 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:02 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/30 05:39:02 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "25/07/30 05:39:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (b7942ec7cbdb,executor driver, partition 0, PROCESS_LOCAL, 10228 bytes) \n",
      "25/07/30 05:39:02 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "25/07/30 05:39:02 INFO CodeGenerator: Code generated in 6.292291 ms\n",
      "25/07/30 05:39:02 INFO FileScanRDD: Reading File path: file:///app/data/tech_filtered.csv, range: 0-148812, partition values: [empty row]\n",
      "25/07/30 05:39:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2019 bytes result sent to driver\n",
      "25/07/30 05:39:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 69 ms on b7942ec7cbdb (executor driver) (1/1)\n",
      "25/07/30 05:39:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:02 INFO DAGScheduler: ResultStage 1 (csv at <unknown>:0) finished in 82 ms\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/30 05:39:02 INFO TaskSchedulerImpl: Canceling stage 1\n",
      "25/07/30 05:39:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Job 1 finished: csv at <unknown>:0, took 82.653707 ms\n",
      "25/07/30 05:39:02 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "25/07/30 05:39:02 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "25/07/30 05:39:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/07/30 05:39:02 INFO FileSourceStrategy: Post-Scan Filters: Set((length(trim(value#33, None)) > 0))\n",
      "25/07/30 05:39:02 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 214.3 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:02 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 38.7 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:02 INFO SparkContext: Created broadcast 4 from csv at <unknown>:0\n",
      "25/07/30 05:39:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12563877 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/30 05:39:02 INFO SparkContext: Starting job: csv at <unknown>:0\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Got job 2 (csv at <unknown>:0) with 1 output partitions\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Final stage: ResultStage 2 (csv at <unknown>:0)\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at csv at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:02 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.3 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:02 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:02 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at csv at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/30 05:39:02 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "25/07/30 05:39:02 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (b7942ec7cbdb,executor driver, partition 0, PROCESS_LOCAL, 10227 bytes) \n",
      "25/07/30 05:39:02 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "25/07/30 05:39:02 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 0-12563877, partition values: [empty row]\n",
      "25/07/30 05:39:02 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1726 bytes result sent to driver\n",
      "25/07/30 05:39:02 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 34 ms on b7942ec7cbdb (executor driver) (1/1)\n",
      "25/07/30 05:39:02 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:02 INFO DAGScheduler: ResultStage 2 (csv at <unknown>:0) finished in 46 ms\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/30 05:39:02 INFO TaskSchedulerImpl: Canceling stage 2\n",
      "25/07/30 05:39:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Job 2 finished: csv at <unknown>:0, took 49.137834 ms\n",
      "25/07/30 05:39:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/07/30 05:39:02 INFO FileSourceStrategy: Post-Scan Filters: Set()\n",
      "25/07/30 05:39:02 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 214.3 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:02 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 38.7 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:02 INFO SparkContext: Created broadcast 6 from csv at <unknown>:0\n",
      "25/07/30 05:39:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12563877 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/30 05:39:02 INFO SparkContext: Starting job: csv at <unknown>:0\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Got job 3 (csv at <unknown>:0) with 8 output partitions\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Final stage: ResultStage 3 (csv at <unknown>:0)\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[19] at csv at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 22.6 KiB, free 434.1 MiB)\n",
      "25/07/30 05:39:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.1 MiB)\n",
      "25/07/30 05:39:02 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:02 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at csv at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/07/30 05:39:02 INFO TaskSchedulerImpl: Adding task set 3.0 with 8 tasks resource profile 0\n",
      "25/07/30 05:39:02 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (b7942ec7cbdb,executor driver, partition 0, PROCESS_LOCAL, 10227 bytes) \n",
      "25/07/30 05:39:02 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4) (b7942ec7cbdb,executor driver, partition 1, PROCESS_LOCAL, 10227 bytes) \n",
      "25/07/30 05:39:02 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 5) (b7942ec7cbdb,executor driver, partition 2, PROCESS_LOCAL, 10227 bytes) \n",
      "25/07/30 05:39:02 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 6) (b7942ec7cbdb,executor driver, partition 3, PROCESS_LOCAL, 10227 bytes) \n",
      "25/07/30 05:39:02 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 7) (b7942ec7cbdb,executor driver, partition 4, PROCESS_LOCAL, 10227 bytes) \n",
      "25/07/30 05:39:02 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 8) (b7942ec7cbdb,executor driver, partition 5, PROCESS_LOCAL, 10227 bytes) \n",
      "25/07/30 05:39:02 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 9) (b7942ec7cbdb,executor driver, partition 6, PROCESS_LOCAL, 10227 bytes) \n",
      "25/07/30 05:39:02 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 10) (b7942ec7cbdb,executor driver, partition 7, PROCESS_LOCAL, 10227 bytes) \n",
      "25/07/30 05:39:02 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "25/07/30 05:39:02 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)\n",
      "25/07/30 05:39:02 INFO Executor: Running task 2.0 in stage 3.0 (TID 5)\n",
      "25/07/30 05:39:02 INFO Executor: Running task 3.0 in stage 3.0 (TID 6)\n",
      "25/07/30 05:39:02 INFO Executor: Running task 4.0 in stage 3.0 (TID 7)\n",
      "25/07/30 05:39:02 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 0-12563877, partition values: [empty row]\n",
      "25/07/30 05:39:02 INFO Executor: Running task 5.0 in stage 3.0 (TID 8)\n",
      "25/07/30 05:39:02 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 25127754-37691631, partition values: [empty row]\n",
      "25/07/30 05:39:02 INFO Executor: Running task 7.0 in stage 3.0 (TID 10)\n",
      "25/07/30 05:39:02 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 12563877-25127754, partition values: [empty row]\n",
      "25/07/30 05:39:02 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 37691631-50255508, partition values: [empty row]\n",
      "25/07/30 05:39:02 INFO Executor: Running task 6.0 in stage 3.0 (TID 9)\n",
      "25/07/30 05:39:02 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 62819385-75383262, partition values: [empty row]\n",
      "25/07/30 05:39:02 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 87947139-96316712, partition values: [empty row]\n",
      "25/07/30 05:39:02 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 75383262-87947139, partition values: [empty row]\n",
      "25/07/30 05:39:02 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 50255508-62819385, partition values: [empty row]\n",
      "25/07/30 05:39:04 INFO Executor: Finished task 7.0 in stage 3.0 (TID 10). 2022 bytes result sent to driver\n",
      "25/07/30 05:39:04 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 10) in 1910 ms on b7942ec7cbdb (executor driver) (1/8)\n",
      "25/07/30 05:39:04 INFO Executor: Finished task 5.0 in stage 3.0 (TID 8). 2033 bytes result sent to driver\n",
      "25/07/30 05:39:04 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 8) in 2192 ms on b7942ec7cbdb (executor driver) (2/8)\n",
      "25/07/30 05:39:04 INFO Executor: Finished task 6.0 in stage 3.0 (TID 9). 1979 bytes result sent to driver\n",
      "25/07/30 05:39:04 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 9) in 2216 ms on b7942ec7cbdb (executor driver) (3/8)\n",
      "25/07/30 05:39:04 INFO Executor: Finished task 3.0 in stage 3.0 (TID 6). 1979 bytes result sent to driver\n",
      "25/07/30 05:39:04 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 6) in 2223 ms on b7942ec7cbdb (executor driver) (4/8)\n",
      "25/07/30 05:39:04 INFO Executor: Finished task 2.0 in stage 3.0 (TID 5). 1979 bytes result sent to driver\n",
      "25/07/30 05:39:04 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 5) in 2235 ms on b7942ec7cbdb (executor driver) (5/8)\n",
      "25/07/30 05:39:04 INFO Executor: Finished task 4.0 in stage 3.0 (TID 7). 1979 bytes result sent to driver\n",
      "25/07/30 05:39:04 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 7) in 2268 ms on b7942ec7cbdb (executor driver) (6/8)\n",
      "25/07/30 05:39:04 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 2036 bytes result sent to driver\n",
      "25/07/30 05:39:04 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 2283 ms on b7942ec7cbdb (executor driver) (7/8)\n",
      "25/07/30 05:39:04 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1979 bytes result sent to driver\n",
      "25/07/30 05:39:04 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2284 ms on b7942ec7cbdb (executor driver) (8/8)\n",
      "25/07/30 05:39:04 INFO TaskSchedulerImpl: Removed TaskSet 3.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:04 INFO DAGScheduler: ResultStage 3 (csv at <unknown>:0) finished in 2296 ms\n",
      "25/07/30 05:39:04 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/30 05:39:04 INFO TaskSchedulerImpl: Canceling stage 3\n",
      "25/07/30 05:39:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "25/07/30 05:39:04 INFO DAGScheduler: Job 3 finished: csv at <unknown>:0, took 2298.986084 ms\n",
      "25/07/30 05:39:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Symbol)\n",
      "25/07/30 05:39:05 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(Symbol#51))\n",
      "25/07/30 05:39:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Symbol)\n",
      "25/07/30 05:39:05 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(Symbol#18))\n",
      "25/07/30 05:39:05 INFO CodeGenerator: Code generated in 6.522625 ms\n",
      "25/07/30 05:39:05 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 214.2 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:05 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 38.6 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:05 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$2 at <unknown>:0\n",
      "25/07/30 05:39:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/30 05:39:05 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at <unknown>:0\n",
      "25/07/30 05:39:05 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) with 1 output partitions\n",
      "25/07/30 05:39:05 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0)\n",
      "25/07/30 05:39:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/30 05:39:05 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:05 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:05 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 17.0 KiB, free 433.9 MiB)\n",
      "25/07/30 05:39:05 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 433.8 MiB)\n",
      "25/07/30 05:39:05 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/30 05:39:05 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "25/07/30 05:39:05 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 11) (b7942ec7cbdb,executor driver, partition 0, PROCESS_LOCAL, 10228 bytes) \n",
      "25/07/30 05:39:05 INFO Executor: Running task 0.0 in stage 4.0 (TID 11)\n",
      "25/07/30 05:39:05 INFO CodeGenerator: Code generated in 5.5515 ms\n",
      "25/07/30 05:39:05 INFO FileScanRDD: Reading File path: file:///app/data/tech_filtered.csv, range: 0-148812, partition values: [empty row]\n",
      "25/07/30 05:39:05 INFO CodeGenerator: Code generated in 5.956958 ms\n",
      "25/07/30 05:39:05 INFO CodeGenerator: Code generated in 6.332708 ms\n",
      "25/07/30 05:39:05 INFO Executor: Finished task 0.0 in stage 4.0 (TID 11). 3242 bytes result sent to driver\n",
      "25/07/30 05:39:05 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 11) in 98 ms on b7942ec7cbdb (executor driver) (1/1)\n",
      "25/07/30 05:39:05 INFO TaskSchedulerImpl: Removed TaskSet 4.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:05 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) finished in 102 ms\n",
      "25/07/30 05:39:05 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/30 05:39:05 INFO TaskSchedulerImpl: Canceling stage 4\n",
      "25/07/30 05:39:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "25/07/30 05:39:05 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$2 at <unknown>:0, took 105.791542 ms\n",
      "25/07/30 05:39:05 INFO CodeGenerator: Code generated in 9.63825 ms\n",
      "25/07/30 05:39:05 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 2.0 MiB, free 431.8 MiB)\n",
      "25/07/30 05:39:05 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 431.8 MiB)\n",
      "25/07/30 05:39:05 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$2 at <unknown>:0\n",
      "25/07/30 05:39:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Symbol)\n",
      "25/07/30 05:39:06 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(Symbol#51))\n",
      "25/07/30 05:39:06 INFO CodeGenerator: Code generated in 9.300083 ms\n",
      "25/07/30 05:39:06 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 214.2 KiB, free 431.6 MiB)\n",
      "25/07/30 05:39:06 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 38.6 KiB, free 431.6 MiB)\n",
      "25/07/30 05:39:06 INFO SparkContext: Created broadcast 11 from $anonfun$withThreadLocalCaptured$2 at <unknown>:0\n",
      "25/07/30 05:39:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12563877 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/30 05:39:06 INFO DAGScheduler: Registering RDD 27 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) as input to shuffle 0\n",
      "25/07/30 05:39:06 INFO DAGScheduler: Got map stage job 5 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) with 8 output partitions\n",
      "25/07/30 05:39:06 INFO DAGScheduler: Final stage: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0)\n",
      "25/07/30 05:39:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/30 05:39:06 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:06 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:06 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 21.9 KiB, free 431.6 MiB)\n",
      "25/07/30 05:39:06 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 431.6 MiB)\n",
      "25/07/30 05:39:06 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:06 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/07/30 05:39:06 INFO TaskSchedulerImpl: Adding task set 5.0 with 8 tasks resource profile 0\n",
      "25/07/30 05:39:06 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 12) (b7942ec7cbdb,executor driver, partition 0, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:06 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 13) (b7942ec7cbdb,executor driver, partition 1, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:06 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 14) (b7942ec7cbdb,executor driver, partition 2, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:06 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 15) (b7942ec7cbdb,executor driver, partition 3, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:06 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 16) (b7942ec7cbdb,executor driver, partition 4, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:06 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 17) (b7942ec7cbdb,executor driver, partition 5, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:06 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 18) (b7942ec7cbdb,executor driver, partition 6, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:06 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 19) (b7942ec7cbdb,executor driver, partition 7, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:06 INFO Executor: Running task 0.0 in stage 5.0 (TID 12)\n",
      "25/07/30 05:39:06 INFO Executor: Running task 3.0 in stage 5.0 (TID 15)\n",
      "25/07/30 05:39:06 INFO Executor: Running task 4.0 in stage 5.0 (TID 16)\n",
      "25/07/30 05:39:06 INFO Executor: Running task 5.0 in stage 5.0 (TID 17)\n",
      "25/07/30 05:39:06 INFO Executor: Running task 6.0 in stage 5.0 (TID 18)\n",
      "25/07/30 05:39:06 INFO Executor: Running task 2.0 in stage 5.0 (TID 14)\n",
      "25/07/30 05:39:06 INFO Executor: Running task 1.0 in stage 5.0 (TID 13)\n",
      "25/07/30 05:39:06 INFO Executor: Running task 7.0 in stage 5.0 (TID 19)\n",
      "25/07/30 05:39:06 INFO CodeGenerator: Code generated in 9.050375 ms\n",
      "25/07/30 05:39:06 INFO CodeGenerator: Code generated in 13.76175 ms\n",
      "25/07/30 05:39:06 INFO SecurityManager: Changing view acls to: spark\n",
      "25/07/30 05:39:06 INFO SecurityManager: Changing modify acls to: spark\n",
      "25/07/30 05:39:06 INFO SecurityManager: Changing view acls groups to: spark\n",
      "25/07/30 05:39:06 INFO SecurityManager: Changing modify acls groups to: spark\n",
      "25/07/30 05:39:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY; RPC SSL disabled\n",
      "25/07/30 05:39:06 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 0-12563877, partition values: [empty row]\n",
      "25/07/30 05:39:06 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 12563877-25127754, partition values: [empty row]\n",
      "25/07/30 05:39:06 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 87947139-96316712, partition values: [empty row]\n",
      "25/07/30 05:39:06 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 75383262-87947139, partition values: [empty row]\n",
      "25/07/30 05:39:06 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 62819385-75383262, partition values: [empty row]\n",
      "25/07/30 05:39:06 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 50255508-62819385, partition values: [empty row]\n",
      "25/07/30 05:39:06 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 37691631-50255508, partition values: [empty row]\n",
      "25/07/30 05:39:06 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 25127754-37691631, partition values: [empty row]\n",
      "25/07/30 05:39:06 INFO CodeGenerator: Code generated in 4.6135 ms\n",
      "25/07/30 05:39:06 INFO CodeGenerator: Code generated in 2.085917 ms\n",
      "25/07/30 05:39:07 INFO Executor: Finished task 7.0 in stage 5.0 (TID 19). 2258 bytes result sent to driver\n",
      "25/07/30 05:39:07 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 19) in 1169 ms on b7942ec7cbdb (executor driver) (1/8)\n",
      "25/07/30 05:39:07 INFO Executor: Finished task 2.0 in stage 5.0 (TID 14). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:07 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 14) in 1560 ms on b7942ec7cbdb (executor driver) (2/8)\n",
      "25/07/30 05:39:07 INFO Executor: Finished task 5.0 in stage 5.0 (TID 17). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:07 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 17) in 1629 ms on b7942ec7cbdb (executor driver) (3/8)\n",
      "25/07/30 05:39:07 INFO Executor: Finished task 3.0 in stage 5.0 (TID 15). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:07 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 15) in 1660 ms on b7942ec7cbdb (executor driver) (4/8)\n",
      "25/07/30 05:39:07 INFO Executor: Finished task 1.0 in stage 5.0 (TID 13). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:07 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 13) in 1711 ms on b7942ec7cbdb (executor driver) (5/8)\n",
      "25/07/30 05:39:07 INFO Executor: Finished task 6.0 in stage 5.0 (TID 18). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:07 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 18) in 1760 ms on b7942ec7cbdb (executor driver) (6/8)\n",
      "25/07/30 05:39:07 INFO Executor: Finished task 0.0 in stage 5.0 (TID 12). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:07 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 12) in 1774 ms on b7942ec7cbdb (executor driver) (7/8)\n",
      "25/07/30 05:39:07 INFO Executor: Finished task 4.0 in stage 5.0 (TID 16). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:07 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 16) in 1778 ms on b7942ec7cbdb (executor driver) (8/8)\n",
      "25/07/30 05:39:07 INFO TaskSchedulerImpl: Removed TaskSet 5.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:07 INFO DAGScheduler: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) finished in 1809 ms\n",
      "25/07/30 05:39:07 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/30 05:39:07 INFO DAGScheduler: running: HashSet()\n",
      "25/07/30 05:39:07 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/30 05:39:07 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/30 05:39:07 INFO ShufflePartitionsUtil: For shuffle(0, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 39.261916 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 6.582625 ms\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Registering RDD 32 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) as input to shuffle 1\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Got map stage job 6 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) with 2 output partitions\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Final stage: ShuffleMapStage 7 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0)\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[32] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:08 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 77.4 KiB, free 431.5 MiB)\n",
      "25/07/30 05:39:08 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 431.5 MiB)\n",
      "25/07/30 05:39:08 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[32] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "25/07/30 05:39:08 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n",
      "25/07/30 05:39:08 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 20) (b7942ec7cbdb,executor driver, partition 0, NODE_LOCAL, 9620 bytes) \n",
      "25/07/30 05:39:08 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 21) (b7942ec7cbdb,executor driver, partition 1, NODE_LOCAL, 9620 bytes) \n",
      "25/07/30 05:39:08 INFO Executor: Running task 0.0 in stage 7.0 (TID 20)\n",
      "25/07/30 05:39:08 INFO Executor: Running task 1.0 in stage 7.0 (TID 21)\n",
      "25/07/30 05:39:08 INFO ShuffleBlockFetcherIterator: Getting 8 (1032.9 KiB) non-empty blocks including 8 (1032.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/30 05:39:08 INFO ShuffleBlockFetcherIterator: Getting 7 (1043.2 KiB) non-empty blocks including 7 (1043.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/30 05:39:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
      "25/07/30 05:39:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 9.104792 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 10.178875 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 3.872333 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 3.757708 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 6.500375 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 4.219333 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 3.4105 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 23.515833 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 3.041917 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 3.297292 ms\n",
      "25/07/30 05:39:08 INFO Executor: Finished task 0.0 in stage 7.0 (TID 20). 5981 bytes result sent to driver\n",
      "25/07/30 05:39:08 INFO Executor: Finished task 1.0 in stage 7.0 (TID 21). 5981 bytes result sent to driver\n",
      "25/07/30 05:39:08 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 20) in 629 ms on b7942ec7cbdb (executor driver) (1/2)\n",
      "25/07/30 05:39:08 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 21) in 629 ms on b7942ec7cbdb (executor driver) (2/2)\n",
      "25/07/30 05:39:08 INFO TaskSchedulerImpl: Removed TaskSet 7.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:08 INFO DAGScheduler: ShuffleMapStage 7 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) finished in 638 ms\n",
      "25/07/30 05:39:08 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/30 05:39:08 INFO DAGScheduler: running: HashSet()\n",
      "25/07/30 05:39:08 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/30 05:39:08 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/30 05:39:08 INFO ShufflePartitionsUtil: For shuffle(1, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 5.088959 ms\n",
      "25/07/30 05:39:08 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 24.14425 ms\n",
      "25/07/30 05:39:08 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at <unknown>:0\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Got job 7 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) with 1 output partitions\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Final stage: ResultStage 10 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0)\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[36] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:08 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 81.2 KiB, free 431.4 MiB)\n",
      "25/07/30 05:39:08 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 32.4 KiB, free 431.4 MiB)\n",
      "25/07/30 05:39:08 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[36] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/30 05:39:08 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "25/07/30 05:39:08 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22) (b7942ec7cbdb,executor driver, partition 0, NODE_LOCAL, 9631 bytes) \n",
      "25/07/30 05:39:08 INFO Executor: Running task 0.0 in stage 10.0 (TID 22)\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 6.773875 ms\n",
      "25/07/30 05:39:08 INFO ShuffleBlockFetcherIterator: Getting 2 (1632.0 B) non-empty blocks including 2 (1632.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/30 05:39:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 11.369833 ms\n",
      "25/07/30 05:39:08 INFO Executor: Finished task 0.0 in stage 10.0 (TID 22). 7554 bytes result sent to driver\n",
      "25/07/30 05:39:08 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 57 ms on b7942ec7cbdb (executor driver) (1/1)\n",
      "25/07/30 05:39:08 INFO TaskSchedulerImpl: Removed TaskSet 10.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:08 INFO DAGScheduler: ResultStage 10 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) finished in 64 ms\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/30 05:39:08 INFO TaskSchedulerImpl: Canceling stage 10\n",
      "25/07/30 05:39:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "25/07/30 05:39:08 INFO DAGScheduler: Job 7 finished: $anonfun$withThreadLocalCaptured$2 at <unknown>:0, took 67.302125 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 7.881541 ms\n",
      "25/07/30 05:39:08 INFO CodeGenerator: Code generated in 4.349125 ms\n",
      "+-----------------------------------+---------------------+--------------------+\n",
      "|Industry                           |AvgReturn            |Volatility          |\n",
      "+-----------------------------------+---------------------+--------------------+\n",
      "|Solar                              |0.0012543932564422512|0.04229120754877891 |\n",
      "|Semiconductors                     |0.0014210097569581825|0.02838869817425058 |\n",
      "|Computer Hardware                  |9.073457723934148E-4 |0.025088068694796368|\n",
      "|Semiconductor Equipment & Materials|0.0010623361351632537|0.02428030549950071 |\n",
      "|Software - Application             |7.531987322759906E-4 |0.020876858387267144|\n",
      "|Software - Infrastructure          |9.264412381495802E-4 |0.020826073812496255|\n",
      "|Communication Equipment            |5.978955084884531E-4 |0.017613262171385535|\n",
      "|Electronic Components              |6.00477284883338E-4  |0.01732610326997517 |\n",
      "+-----------------------------------+---------------------+--------------------+\n",
      "\n",
      "25/07/30 05:39:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Symbol)\n",
      "25/07/30 05:39:09 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(Symbol#51))\n",
      "25/07/30 05:39:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Symbol)\n",
      "25/07/30 05:39:09 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(Symbol#18))\n",
      "25/07/30 05:39:09 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 214.2 KiB, free 431.2 MiB)\n",
      "25/07/30 05:39:09 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 38.6 KiB, free 431.1 MiB)\n",
      "25/07/30 05:39:09 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$2 at <unknown>:0\n",
      "25/07/30 05:39:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/30 05:39:09 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at <unknown>:0\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) with 1 output partitions\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0)\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[40] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:09 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 17.0 KiB, free 431.1 MiB)\n",
      "25/07/30 05:39:09 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 431.1 MiB)\n",
      "25/07/30 05:39:09 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[40] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/30 05:39:09 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "25/07/30 05:39:09 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 23) (b7942ec7cbdb,executor driver, partition 0, PROCESS_LOCAL, 10228 bytes) \n",
      "25/07/30 05:39:09 INFO Executor: Running task 0.0 in stage 11.0 (TID 23)\n",
      "25/07/30 05:39:09 INFO FileScanRDD: Reading File path: file:///app/data/tech_filtered.csv, range: 0-148812, partition values: [empty row]\n",
      "25/07/30 05:39:09 INFO Executor: Finished task 0.0 in stage 11.0 (TID 23). 3242 bytes result sent to driver\n",
      "25/07/30 05:39:09 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 23) in 11 ms on b7942ec7cbdb (executor driver) (1/1)\n",
      "25/07/30 05:39:09 INFO TaskSchedulerImpl: Removed TaskSet 11.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:09 INFO DAGScheduler: ResultStage 11 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) finished in 16 ms\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/30 05:39:09 INFO TaskSchedulerImpl: Canceling stage 11\n",
      "25/07/30 05:39:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Job 8 finished: $anonfun$withThreadLocalCaptured$2 at <unknown>:0, took 17.645291 ms\n",
      "25/07/30 05:39:09 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 2.0 MiB, free 429.1 MiB)\n",
      "25/07/30 05:39:09 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 429.1 MiB)\n",
      "25/07/30 05:39:09 INFO SparkContext: Created broadcast 17 from $anonfun$withThreadLocalCaptured$2 at <unknown>:0\n",
      "25/07/30 05:39:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Symbol)\n",
      "25/07/30 05:39:09 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(Symbol#51))\n",
      "25/07/30 05:39:09 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 214.2 KiB, free 428.9 MiB)\n",
      "25/07/30 05:39:09 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 38.6 KiB, free 429.0 MiB)\n",
      "25/07/30 05:39:09 INFO SparkContext: Created broadcast 18 from $anonfun$withThreadLocalCaptured$2 at <unknown>:0\n",
      "25/07/30 05:39:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12563877 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Registering RDD 44 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) as input to shuffle 2\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Got map stage job 9 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) with 8 output partitions\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Final stage: ShuffleMapStage 12 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0)\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[44] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:09 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 21.9 KiB, free 428.9 MiB)\n",
      "25/07/30 05:39:09 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 428.9 MiB)\n",
      "25/07/30 05:39:09 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:09 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[44] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/07/30 05:39:09 INFO TaskSchedulerImpl: Adding task set 12.0 with 8 tasks resource profile 0\n",
      "25/07/30 05:39:09 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 24) (b7942ec7cbdb,executor driver, partition 0, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:09 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 25) (b7942ec7cbdb,executor driver, partition 1, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:09 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 26) (b7942ec7cbdb,executor driver, partition 2, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:09 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 27) (b7942ec7cbdb,executor driver, partition 3, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:09 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 28) (b7942ec7cbdb,executor driver, partition 4, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:09 INFO TaskSetManager: Starting task 5.0 in stage 12.0 (TID 29) (b7942ec7cbdb,executor driver, partition 5, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:09 INFO TaskSetManager: Starting task 6.0 in stage 12.0 (TID 30) (b7942ec7cbdb,executor driver, partition 6, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:09 INFO TaskSetManager: Starting task 7.0 in stage 12.0 (TID 31) (b7942ec7cbdb,executor driver, partition 7, PROCESS_LOCAL, 10216 bytes) \n",
      "25/07/30 05:39:09 INFO Executor: Running task 2.0 in stage 12.0 (TID 26)\n",
      "25/07/30 05:39:09 INFO Executor: Running task 0.0 in stage 12.0 (TID 24)\n",
      "25/07/30 05:39:09 INFO Executor: Running task 3.0 in stage 12.0 (TID 27)\n",
      "25/07/30 05:39:09 INFO Executor: Running task 7.0 in stage 12.0 (TID 31)\n",
      "25/07/30 05:39:09 INFO Executor: Running task 1.0 in stage 12.0 (TID 25)\n",
      "25/07/30 05:39:09 INFO Executor: Running task 4.0 in stage 12.0 (TID 28)\n",
      "25/07/30 05:39:09 INFO Executor: Running task 5.0 in stage 12.0 (TID 29)\n",
      "25/07/30 05:39:09 INFO Executor: Running task 6.0 in stage 12.0 (TID 30)\n",
      "25/07/30 05:39:09 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 62819385-75383262, partition values: [empty row]\n",
      "25/07/30 05:39:09 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 37691631-50255508, partition values: [empty row]\n",
      "25/07/30 05:39:09 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 75383262-87947139, partition values: [empty row]\n",
      "25/07/30 05:39:09 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 0-12563877, partition values: [empty row]\n",
      "25/07/30 05:39:09 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 25127754-37691631, partition values: [empty row]\n",
      "25/07/30 05:39:09 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 87947139-96316712, partition values: [empty row]\n",
      "25/07/30 05:39:09 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 12563877-25127754, partition values: [empty row]\n",
      "25/07/30 05:39:09 INFO FileScanRDD: Reading File path: file:///app/data/sp500_stocks.csv, range: 50255508-62819385, partition values: [empty row]\n",
      "25/07/30 05:39:09 INFO Executor: Finished task 7.0 in stage 12.0 (TID 31). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:09 INFO TaskSetManager: Finished task 7.0 in stage 12.0 (TID 31) in 457 ms on b7942ec7cbdb (executor driver) (1/8)\n",
      "25/07/30 05:39:10 INFO Executor: Finished task 2.0 in stage 12.0 (TID 26). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:10 INFO Executor: Finished task 5.0 in stage 12.0 (TID 29). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Finished task 5.0 in stage 12.0 (TID 29) in 864 ms on b7942ec7cbdb (executor driver) (2/8)\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 26) in 865 ms on b7942ec7cbdb (executor driver) (3/8)\n",
      "25/07/30 05:39:10 INFO Executor: Finished task 3.0 in stage 12.0 (TID 27). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 27) in 876 ms on b7942ec7cbdb (executor driver) (4/8)\n",
      "25/07/30 05:39:10 INFO Executor: Finished task 4.0 in stage 12.0 (TID 28). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 28) in 880 ms on b7942ec7cbdb (executor driver) (5/8)\n",
      "25/07/30 05:39:10 INFO Executor: Finished task 6.0 in stage 12.0 (TID 30). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Finished task 6.0 in stage 12.0 (TID 30) in 959 ms on b7942ec7cbdb (executor driver) (6/8)\n",
      "25/07/30 05:39:10 INFO Executor: Finished task 0.0 in stage 12.0 (TID 24). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 24) in 963 ms on b7942ec7cbdb (executor driver) (7/8)\n",
      "25/07/30 05:39:10 INFO Executor: Finished task 1.0 in stage 12.0 (TID 25). 2215 bytes result sent to driver\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 25) in 966 ms on b7942ec7cbdb (executor driver) (8/8)\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Removed TaskSet 12.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:10 INFO DAGScheduler: ShuffleMapStage 12 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) finished in 969 ms\n",
      "25/07/30 05:39:10 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/30 05:39:10 INFO DAGScheduler: running: HashSet()\n",
      "25/07/30 05:39:10 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/30 05:39:10 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/30 05:39:10 INFO ShufflePartitionsUtil: For shuffle(2, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Registering RDD 49 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) as input to shuffle 3\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Got map stage job 10 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) with 2 output partitions\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Final stage: ShuffleMapStage 14 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0)\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[49] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:10 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 77.8 KiB, free 429.0 MiB)\n",
      "25/07/30 05:39:10 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 32.2 KiB, free 429.0 MiB)\n",
      "25/07/30 05:39:10 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[49] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 32) (b7942ec7cbdb,executor driver, partition 0, NODE_LOCAL, 9620 bytes) \n",
      "25/07/30 05:39:10 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 33) (b7942ec7cbdb,executor driver, partition 1, NODE_LOCAL, 9620 bytes) \n",
      "25/07/30 05:39:10 INFO Executor: Running task 0.0 in stage 14.0 (TID 32)\n",
      "25/07/30 05:39:10 INFO Executor: Running task 1.0 in stage 14.0 (TID 33)\n",
      "25/07/30 05:39:10 INFO ShuffleBlockFetcherIterator: Getting 7 (1043.2 KiB) non-empty blocks including 7 (1043.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/30 05:39:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/07/30 05:39:10 INFO ShuffleBlockFetcherIterator: Getting 8 (1032.9 KiB) non-empty blocks including 8 (1032.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/30 05:39:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/07/30 05:39:10 INFO Executor: Finished task 1.0 in stage 14.0 (TID 33). 5938 bytes result sent to driver\n",
      "25/07/30 05:39:10 INFO Executor: Finished task 0.0 in stage 14.0 (TID 32). 5938 bytes result sent to driver\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 33) in 165 ms on b7942ec7cbdb (executor driver) (1/2)\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 32) in 165 ms on b7942ec7cbdb (executor driver) (2/2)\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Removed TaskSet 14.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:10 INFO DAGScheduler: ShuffleMapStage 14 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) finished in 170 ms\n",
      "25/07/30 05:39:10 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/30 05:39:10 INFO DAGScheduler: running: HashSet()\n",
      "25/07/30 05:39:10 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/30 05:39:10 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/30 05:39:10 INFO ShufflePartitionsUtil: For shuffle(3, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/30 05:39:10 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/30 05:39:10 INFO CodeGenerator: Code generated in 11.276375 ms\n",
      "25/07/30 05:39:10 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at <unknown>:0\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Got job 11 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) with 1 output partitions\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Final stage: ResultStage 17 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0)\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[54] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:10 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 83.5 KiB, free 428.9 MiB)\n",
      "25/07/30 05:39:10 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 33.0 KiB, free 428.8 MiB)\n",
      "25/07/30 05:39:10 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[54] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 34) (b7942ec7cbdb,executor driver, partition 0, NODE_LOCAL, 9631 bytes) \n",
      "25/07/30 05:39:10 INFO Executor: Running task 0.0 in stage 17.0 (TID 34)\n",
      "25/07/30 05:39:10 INFO ShuffleBlockFetcherIterator: Getting 2 (1632.0 B) non-empty blocks including 2 (1632.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/30 05:39:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/07/30 05:39:10 INFO CodeGenerator: Code generated in 4.442083 ms\n",
      "25/07/30 05:39:10 INFO Executor: Finished task 0.0 in stage 17.0 (TID 34). 7390 bytes result sent to driver\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 34) in 35 ms on b7942ec7cbdb (executor driver) (1/1)\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Removed TaskSet 17.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:10 INFO DAGScheduler: ResultStage 17 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) finished in 39 ms\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Canceling stage 17\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Job 11 finished: $anonfun$withThreadLocalCaptured$2 at <unknown>:0, took 41.419917 ms\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Registering RDD 55 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) as input to shuffle 4\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Got map stage job 12 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) with 1 output partitions\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Final stage: ShuffleMapStage 20 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0)\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[55] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:10 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 83.7 KiB, free 428.8 MiB)\n",
      "25/07/30 05:39:10 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 33.1 KiB, free 428.7 MiB)\n",
      "25/07/30 05:39:10 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[55] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 35) (b7942ec7cbdb,executor driver, partition 0, NODE_LOCAL, 9620 bytes) \n",
      "25/07/30 05:39:10 INFO Executor: Running task 0.0 in stage 20.0 (TID 35)\n",
      "25/07/30 05:39:10 INFO CodeGenerator: Code generated in 4.273625 ms\n",
      "25/07/30 05:39:10 INFO ShuffleBlockFetcherIterator: Getting 2 (1632.0 B) non-empty blocks including 2 (1632.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/30 05:39:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/07/30 05:39:10 INFO Executor: Finished task 0.0 in stage 20.0 (TID 35). 7135 bytes result sent to driver\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 35) in 35 ms on b7942ec7cbdb (executor driver) (1/1)\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Removed TaskSet 20.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:10 INFO DAGScheduler: ShuffleMapStage 20 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) finished in 42 ms\n",
      "25/07/30 05:39:10 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/30 05:39:10 INFO DAGScheduler: running: HashSet()\n",
      "25/07/30 05:39:10 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/30 05:39:10 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/30 05:39:10 INFO ShufflePartitionsUtil: For shuffle(4, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/30 05:39:10 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "25/07/30 05:39:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/07/30 05:39:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/07/30 05:39:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "25/07/30 05:39:10 INFO CodeGenerator: Code generated in 10.353208 ms\n",
      "25/07/30 05:39:10 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at <unknown>:0\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Got job 13 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) with 1 output partitions\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Final stage: ResultStage 24 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0)\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[59] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0), which has no missing parents\n",
      "25/07/30 05:39:10 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 297.3 KiB, free 428.4 MiB)\n",
      "25/07/30 05:39:10 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 108.6 KiB, free 428.3 MiB)\n",
      "25/07/30 05:39:10 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1676\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[59] at $anonfun$withThreadLocalCaptured$2 at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 36) (b7942ec7cbdb,executor driver, partition 0, NODE_LOCAL, 9978 bytes) \n",
      "25/07/30 05:39:10 INFO Executor: Running task 0.0 in stage 24.0 (TID 36)\n",
      "25/07/30 05:39:10 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "25/07/30 05:39:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/07/30 05:39:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/07/30 05:39:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "25/07/30 05:39:10 INFO ShuffleBlockFetcherIterator: Getting 1 (843.0 B) non-empty blocks including 1 (843.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/30 05:39:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/07/30 05:39:10 INFO CodeGenerator: Code generated in 7.046333 ms\n",
      "25/07/30 05:39:10 INFO CodeGenerator: Code generated in 7.983959 ms\n",
      "25/07/30 05:39:10 INFO FileOutputCommitter: Saved output of task 'attempt_202507300539104607802559678239627_0024_m_000000_36' to file:/app/output/industry_metrics/_temporary/0/task_202507300539104607802559678239627_0024_m_000000\n",
      "25/07/30 05:39:10 INFO SparkHadoopMapRedUtil: attempt_202507300539104607802559678239627_0024_m_000000_36: Committed. Elapsed time: 0 ms.\n",
      "25/07/30 05:39:10 INFO Executor: Finished task 0.0 in stage 24.0 (TID 36). 9447 bytes result sent to driver\n",
      "25/07/30 05:39:10 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 36) in 169 ms on b7942ec7cbdb (executor driver) (1/1)\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Removed TaskSet 24.0 whose tasks have all completed, from pool \n",
      "25/07/30 05:39:10 INFO DAGScheduler: ResultStage 24 ($anonfun$withThreadLocalCaptured$2 at <unknown>:0) finished in 206 ms\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Canceling stage 24\n",
      "25/07/30 05:39:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
      "25/07/30 05:39:10 INFO DAGScheduler: Job 13 finished: $anonfun$withThreadLocalCaptured$2 at <unknown>:0, took 209.796708 ms\n",
      "25/07/30 05:39:10 INFO FileFormatWriter: Start to commit write Job 811ae154-a15f-421e-b333-e8899094a77c.\n",
      "25/07/30 05:39:10 INFO FileFormatWriter: Write Job 811ae154-a15f-421e-b333-e8899094a77c committed. Elapsed time: 26 ms.\n",
      "25/07/30 05:39:10 INFO FileFormatWriter: Finished processing stats for write job 811ae154-a15f-421e-b333-e8899094a77c.\n",
      "25/07/30 05:39:11 INFO SparkContext: SparkContext is stopping with exitCode 0 from stop at <unknown>:0.\n",
      "25/07/30 05:39:11 INFO SparkUI: Stopped Spark web UI at http://b7942ec7cbdb:4041\n",
      "25/07/30 05:39:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "25/07/30 05:39:11 INFO MemoryStore: MemoryStore cleared\n",
      "25/07/30 05:39:11 INFO BlockManager: BlockManager stopped\n",
      "25/07/30 05:39:11 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "25/07/30 05:39:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "25/07/30 05:39:11 INFO SparkContext: Successfully stopped SparkContext\n",
      "25/07/30 05:39:11 INFO ShutdownHookManager: Shutdown hook called\n",
      "25/07/30 05:39:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-70efafc9-4926-4c50-8a33-a737c258dd8c/pyspark-4231fc10-9625-48fc-9c38-a409d1350a70\n",
      "25/07/30 05:39:11 INFO ShutdownHookManager: Deleting directory /app/artifacts/spark-a7207777-b630-46a1-80c0-478484214246\n",
      "25/07/30 05:39:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-85021930-50e8-424e-9df2-23bfd8858c10\n",
      "25/07/30 05:39:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-70efafc9-4926-4c50-8a33-a737c258dd8c\n"
     ]
    }
   ],
   "source": [
    "!spark-submit scripts/spark_analysis.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e510a41-7b8f-4a17-bb5f-94ef0ca536f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p failed for path /.config/matplotlib: [Errno 13] Permission denied: '/.config'\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-t1r2nr5q because there was an issue with the default path (/.config/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "Starting plot generation...\n",
      "Ensured 'plots/' directory exists.\n",
      "Reading CSV from: output/industry_metrics/part-00000-ce3657ce-e26c-43df-8f4b-b8a49e9ca83f-c000.csv\n",
      "Data preview:\n",
      "                              Industry  AvgReturn  Volatility\n",
      "0                                Solar   0.001254    0.042291\n",
      "1                       Semiconductors   0.001421    0.028389\n",
      "2                    Computer Hardware   0.000907    0.025088\n",
      "3  Semiconductor Equipment & Materials   0.001062    0.024280\n",
      "4               Software - Application   0.000753    0.020877\n",
      "/app/scripts/generate_plots.py:34: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"Volatility\", y=\"Industry\", data=df_sorted, palette=\"rocket\")\n",
      "Saving volatility_by_industry.png...\n",
      "Saving risk_vs_return.png...\n",
      "Plot generation complete. Check the 'plots/' directory for images.\n"
     ]
    }
   ],
   "source": [
    "!python3 scripts/generate_plots.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626db37e-315c-41e7-a092-c02a4cb82c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
